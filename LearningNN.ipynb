{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic tutorial Jupyter notebook for learning pytorch\n",
    "\n",
    "*Goal*: To implement a neural network that can accept primary vote data and predict and 2CP including who the final two are, preferably outputting \"nice\" distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A basic neural network training on MNIST\n",
    "\n",
    "Online tutorial from [Here](https://www.youtube.com/watch?v=BzcBsTou0C0&ab_channel=sentdex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above data is in a nice form that this utility function can be used conveniently.\n",
    "trainset = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 4, 3, 4, 2, 0, 7, 0, 6, 2])]\n"
     ]
    }
   ],
   "source": [
    "# Testing our that we can read out data\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressing one particular value in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a nice example\n",
    "x,y = data[0][0], data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24fdd606bc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x.view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the basic neural network.\n",
    "- Note that importing ```torch.nn``` and ```torch.nn.functional``` are used together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a neural net, which is represetned as a class.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64) ### !!! IMPORTANT 28*28 comes from the input data, which we known will be a flattened 28x28 pixel image will have be an array of length 28*28 and the 64 is the number of neuorons per layer.\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The net is now initialised, but not trained. Let's test that we can pass data through.\n",
    "X = torch.rand((28,28)) # Represents a random 28x28 pixel image.\n",
    "X = X.view(-1,28*28) # For Neural network to run we need to flatten the data, the -1 tells pytorch to that the number of sample is irrelevant and the 28*28 specificies a tensor of length of 28*28, expected by net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3806, -2.3039, -2.3644, -2.2033, -2.2414, -2.2290, -2.3666, -2.2514,\n",
       "         -2.2840, -2.4268]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X) # Pass X through the net and it will spit out a vector of (logarathmic) probability for whether the is 0-9.\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an optimizer for out neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0269, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.2817, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1,28*28)) # Here is our final parse of the data to give it the correct form befor the net handles it.\n",
    "        loss = F.nll_loss(output,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.96\n"
     ]
    }
   ],
   "source": [
    "# Let's test our data!\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X,y = data\n",
    "        output = net(X.view(-1,28*28))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total +=1 \n",
    "print(\"Accuracy: \",round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[5].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.argmax(net(X[5].view(-1,28*28)))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network and Learning Datahandling\n",
    "\n",
    "*Goal*: To develop a neural network that uses convolutions instead of Linear Layers and get familiar with data preprocessing.\n",
    "\n",
    "- Side idea worth remembering: Convolutional Networks are for multi-dimensional input data. Usually for images, but could be used for 3D models or other data which can be thought of as having \"features\".\n",
    "- Still following online tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data handling.\n",
    "- Take folder of photos nad proccess. Learn to use flags and numpy to process and save data that can then be reloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "REBUILD_DATA = False\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog/\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "    catcount=0\n",
    "    dogcount=0 \n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                try:\n",
    "                    path = os.path.join(label,f)\n",
    "                    img = cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "                    img = cv2.resize(img, (self.IMG_SIZE,self.IMG_SIZE))\n",
    "                    self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])\n",
    "                \n",
    "                    if label == self.CATS:\n",
    "                        self.catcount += 1\n",
    "                    elif label == self.DOGS:\n",
    "                        self.dogcount += 1\n",
    "                except Exception as e:\n",
    "                    #print(str(e))\n",
    "                    pass\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\",self.training_data)\n",
    "        print(\"Cats: \",self.catcount)\n",
    "        print(\"Dogs: \",self.dogcount)\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load(\"training_data.npy\",allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(training_data[1][0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developing the actual CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,5)\n",
    "        self.conv2 = nn.Conv2d(32,64,5)\n",
    "        self.conv3 = nn.Conv2d(64,128,5)\n",
    "\n",
    "        x= torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        self.fc1 = nn.Linear(self._to_linear,512)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "    def convs(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)),(2,2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            print(x[0].shape)\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "    def forward(self,x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1,self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Local\\Temp\\ipykernel_19204\\254632636.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0\n",
    "\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22452\n",
      "2494\n"
     ]
    }
   ],
   "source": [
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(len(train_X))\n",
    "print(len(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 3\n",
    "def train(net):\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0,len(train_X),BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch:{epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:25<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0. Loss: 0.2665332555770874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:25<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1. Loss: 0.21660710871219635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:25<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2. Loss: 0.20716306567192078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i])\n",
    "\n",
    "            net_out = net(test_X[i].view(-1,1,50,50))[0]\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            if predicted_class == real_class:\n",
    "                correct +=1 \n",
    "            total +=1\n",
    "    print(\"Accuracy:\",round(correct/total,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brief Aside On GPU Assisted pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "net= Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:06<00:00, 36.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0. Loss: 0.25140810012817383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:02<00:00, 78.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1. Loss: 0.25140810012817383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 225/225 [00:02<00:00, 78.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2. Loss: 0.25140810012817383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(net):\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0,len(train_X),BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device),batch_y.to(device)\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch:{epoch}. Loss: {loss}\")\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [00:01<00:00, 1374.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "\n",
    "            net_out = net(test_X[i].view(-1,1,50,50).to(device))[0]\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            if predicted_class == real_class:\n",
    "                correct +=1 \n",
    "            total +=1\n",
    "    print(\"Accuracy:\",round(correct/total,3))\n",
    "test(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
